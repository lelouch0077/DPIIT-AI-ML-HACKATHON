{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ae93bc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d4f0acba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class HandwritingDataset(Dataset):\n",
    "    def __init__(self, csv_file, vocab, img_height=32, max_width=512):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.vocab = vocab\n",
    "        self.char2idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "        self.img_height = img_height\n",
    "        self.max_width = max_width\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),  # Converts to [0,1] and shape [C,H,W]\n",
    "            transforms.Normalize((0.5,), (0.5,))  # Normalize grayscale\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def encode_label(self, text):\n",
    "        return [self.char2idx[char] for char in text if char in self.char2idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        img_path = row['FILENAME']\n",
    "        label = str(row['IDENTITY'])\n",
    "\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # [H, W]\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Image not found: {img_path}\")\n",
    "\n",
    "        # Resize to height and pad width if needed\n",
    "        h, w = img.shape\n",
    "        new_w = int(self.img_height * w / h)\n",
    "        new_w = min(new_w, self.max_width)\n",
    "        img_resized = cv2.resize(img, (new_w, self.img_height))  # [H, W]\n",
    "\n",
    "        # Pad to max_width\n",
    "        padded_img = 255 * np.ones((self.img_height, self.max_width), dtype=np.uint8)\n",
    "        padded_img[:, :new_w] = img_resized\n",
    "\n",
    "        img_tensor = self.transform(padded_img)  # [1, H, W]\n",
    "        label_encoded = self.encode_label(label)\n",
    "        label_encoded = torch.tensor(label_encoded, dtype=torch.long)\n",
    "        label_length = torch.tensor(len(label_encoded), dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'image': img_tensor,\n",
    "            'label_encoded': label_encoded,\n",
    "            'label_lengths': label_length,\n",
    "            'label_text': label,\n",
    "            'filename': img_path\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2cdbe438",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(nn.Module):\n",
    "    def __init__(self, img_h, n_channels, n_classes):\n",
    "        super(CRNN, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_channels, 64, 3, 1, 1),  # [B, 64, H, W]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),                 # [B, 64, H/2, W/2]\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),        # [B, 128, H/2, W/2]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),                 # [B, 128, H/4, W/4]\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1)),               # [B, 256, H/8, W/4]\n",
    "\n",
    "            nn.Conv2d(256, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1)),               # [B, 512, H/16, W/4]\n",
    "\n",
    "            nn.Conv2d(512, 512, 2, 1, 0),       # [B, 512, H/16 - 1, W/4 - 1]\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.rnn = nn.Sequential(\n",
    "            nn.LSTM(512, 256, num_layers=2, bidirectional=True, batch_first=True)\n",
    "        )\n",
    "\n",
    "        self.dense = nn.Linear(512, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv = self.cnn(x)  # [B, 512, H, W]\n",
    "        b, c, h, w = conv.size()\n",
    "        assert h == 1, \"the height of conv must be 1\"\n",
    "        conv = conv.squeeze(2)  # remove height dim -> [B, 512, W]\n",
    "        conv = conv.permute(0, 2, 1)  # [B, W, 512] for RNN\n",
    "\n",
    "        rnn_out, _ = self.rnn(conv)  # [B, W, 512]\n",
    "        out = self.dense(rnn_out)   # [B, W, n_classes]\n",
    "        out = out.permute(1, 0, 2)  # for CTC loss: [W, B, n_classes]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "974f1ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crnn_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch: list of items returned by __getitem__()\n",
    "    \"\"\"\n",
    "    images = [item['image'] for item in batch]\n",
    "    labels = [item['label_encoded'] for item in batch]\n",
    "    label_lengths = torch.tensor([len(l) for l in labels], dtype=torch.long)\n",
    "\n",
    "    # Stack images into [B, 1, H, W]\n",
    "    images = torch.stack(images)\n",
    "\n",
    "    # Concatenate labels into a 1D tensor as required by CTC Loss\n",
    "    labels_concat = torch.cat(labels, dim=0)\n",
    "\n",
    "    return {\n",
    "        'images': images,                      # [B, 1, H, W]\n",
    "        'labels': labels_concat,               # [Total number of label tokens]\n",
    "        'label_lengths': label_lengths,        # [B]\n",
    "        'label_text': [item['label_text'] for item in batch],  # For debugging\n",
    "        'input_lengths': torch.full(size=(len(images),), fill_value=images.shape[-1] // 4, dtype=torch.long)\n",
    "        # input_lengths assumes CNN downscales W by 4x\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "55c7b6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(\"abcdefghijklmnopqrstuvwxyz0123456789\")\n",
    "csv_file=r\"C:\\Users\\Raihan\\OneDrive\\Desktop\\DPIIT HACKATHON\\cvsi_fullpath.csv\"\n",
    "dataset = HandwritingDataset(csv_file=csv_file, vocab=vocab)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True,collate_fn=crnn_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5f47f15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CRNN(img_h=32, n_channels=1, n_classes=len(vocab) + 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c2698983",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CTCLoss(blank=len(vocab), zero_infinity=True)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3,weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ff12507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=torch.amp.GradScaler(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38cf142",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raihan\\AppData\\Local\\Temp\\ipykernel_18316\\4249023491.py:1: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "  0%|          | 0/10343 [00:00<?, ?it/s]C:\\Users\\Raihan\\AppData\\Local\\Temp\\ipykernel_18316\\4249023491.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "  0%|          | 0/10343 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "full() received an invalid combination of arguments - got (size=int, fill_value=int, dtype=torch.dtype, ), but expected one of:\n * (tuple of ints size, Number fill_value, *, tuple of names names, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, Number fill_value, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m outputs = outputs.permute(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m)   \n\u001b[32m     18\u001b[39m outputs=outputs.float()\u001b[38;5;66;03m# CTC expects [T, B, C]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m input_lengths = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m\u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# <-- use batch_size here!\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[43m\u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# T\u001b[39;49;00m\n\u001b[32m     23\u001b[39m \u001b[43m\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlong\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(target_lengths))\n\u001b[32m     27\u001b[39m loss = criterion(outputs, targets, input_lengths, target_lengths)\n",
      "\u001b[31mTypeError\u001b[39m: full() received an invalid combination of arguments - got (size=int, fill_value=int, dtype=torch.dtype, ), but expected one of:\n * (tuple of ints size, Number fill_value, *, tuple of names names, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, Number fill_value, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n"
     ]
    }
   ],
   "source": [
    "scaler = GradScaler()\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        images = batch['images'].to(device)                  # [B, 1, H, W]\n",
    "        targets = batch['labels'].to(device)                 # 1D flattened labels\n",
    "        target_lengths = batch['label_lengths'].to(device)   # actual lengths of labels\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast('cuda'):\n",
    "            outputs = model(images)                          # [B, T, C]\n",
    "            outputs = outputs.permute(1, 0, 2)               # CTC expects [T, B, C]\n",
    "\n",
    "            input_lengths = torch.full(\n",
    "                size=(outputs.size(1),),                     # batch size\n",
    "                fill_value=outputs.size(0),                  # all have same T\n",
    "                dtype=torch.long\n",
    "            ).to(device)\n",
    "\n",
    "            loss = criterion(outputs, targets, input_lengths, target_lengths)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258a7015",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
